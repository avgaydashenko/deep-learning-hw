{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=False):\n",
    "        # Tip: if you are training this on AWS the best way is to turn off rendering\n",
    "        # and load it later with the serialized model\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(25, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'),\n",
    "            Dense(25, activation='relu', kernel_initializer='he_uniform'),\n",
    "            Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'),\n",
    "        ])\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            # print(len(self.memory))\n",
    "\n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.discount_factor * \\\n",
    "                                          np.amax(self.target_model.predict(next_state)[0])\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        # You can create a minibatch of the correct target answer and the current value of your own,\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "ACTION_SIZE = 2\n",
    "# actions_num = env.action_space.n\n",
    "# agent = DeepQAgent(state_size, actions_num)\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE)\n",
    "# agent.load_model(\"./save_model/<your_saved_model_name>\")\n",
    "scores, episodes = [], []\n",
    "N_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -200.0   memory length: 200   epsilon: 0.9960200000000077\n",
      "episode: 1   score: -200.0   memory length: 400   epsilon: 0.9920400000000154\n",
      "episode: 2   score: -200.0   memory length: 600   epsilon: 0.988060000000023\n",
      "episode: 3   score: -200.0   memory length: 800   epsilon: 0.9840800000000307\n",
      "episode: 4   score: -200.0   memory length: 1000   epsilon: 0.9801000000000384\n",
      "episode: 5   score: -200.0   memory length: 1200   epsilon: 0.9761200000000461\n",
      "episode: 6   score: -200.0   memory length: 1400   epsilon: 0.9721400000000537\n",
      "episode: 7   score: -200.0   memory length: 1600   epsilon: 0.9681600000000614\n",
      "episode: 8   score: -200.0   memory length: 1800   epsilon: 0.9641800000000691\n",
      "episode: 9   score: -200.0   memory length: 2000   epsilon: 0.9602000000000768\n",
      "episode: 10   score: -200.0   memory length: 2200   epsilon: 0.9562200000000844\n",
      "episode: 11   score: -200.0   memory length: 2400   epsilon: 0.9522400000000921\n",
      "episode: 12   score: -200.0   memory length: 2600   epsilon: 0.9482600000000998\n",
      "episode: 13   score: -200.0   memory length: 2800   epsilon: 0.9442800000001075\n",
      "episode: 14   score: -200.0   memory length: 3000   epsilon: 0.9403000000001152\n",
      "episode: 15   score: -200.0   memory length: 3200   epsilon: 0.9363200000001228\n",
      "episode: 16   score: -200.0   memory length: 3400   epsilon: 0.9323400000001305\n",
      "episode: 17   score: -200.0   memory length: 3600   epsilon: 0.9283600000001382\n",
      "episode: 18   score: -200.0   memory length: 3800   epsilon: 0.9243800000001459\n",
      "episode: 19   score: -200.0   memory length: 4000   epsilon: 0.9204000000001535\n",
      "episode: 20   score: -200.0   memory length: 4200   epsilon: 0.9164200000001612\n",
      "episode: 21   score: -200.0   memory length: 4400   epsilon: 0.9124400000001689\n",
      "episode: 22   score: -200.0   memory length: 4600   epsilon: 0.9084600000001766\n",
      "episode: 23   score: -200.0   memory length: 4800   epsilon: 0.9044800000001842\n",
      "episode: 24   score: -200.0   memory length: 5000   epsilon: 0.9005000000001919\n",
      "episode: 25   score: -200.0   memory length: 5200   epsilon: 0.8965200000001996\n",
      "episode: 26   score: -200.0   memory length: 5400   epsilon: 0.8925400000002073\n",
      "episode: 27   score: -200.0   memory length: 5600   epsilon: 0.888560000000215\n",
      "episode: 28   score: -200.0   memory length: 5800   epsilon: 0.8845800000002226\n",
      "episode: 29   score: -200.0   memory length: 6000   epsilon: 0.8806000000002303\n",
      "episode: 30   score: -200.0   memory length: 6200   epsilon: 0.876620000000238\n",
      "episode: 31   score: -200.0   memory length: 6400   epsilon: 0.8726400000002457\n",
      "episode: 32   score: -200.0   memory length: 6600   epsilon: 0.8686600000002533\n",
      "episode: 33   score: -200.0   memory length: 6800   epsilon: 0.864680000000261\n",
      "episode: 34   score: -200.0   memory length: 7000   epsilon: 0.8607000000002687\n",
      "episode: 35   score: -200.0   memory length: 7200   epsilon: 0.8567200000002764\n",
      "episode: 36   score: -200.0   memory length: 7400   epsilon: 0.852740000000284\n",
      "episode: 37   score: -200.0   memory length: 7600   epsilon: 0.8487600000002917\n",
      "episode: 38   score: -200.0   memory length: 7800   epsilon: 0.8447800000002994\n",
      "episode: 39   score: -200.0   memory length: 8000   epsilon: 0.8408000000003071\n",
      "episode: 40   score: -200.0   memory length: 8200   epsilon: 0.8368200000003148\n",
      "episode: 41   score: -200.0   memory length: 8400   epsilon: 0.8328400000003224\n",
      "episode: 42   score: -200.0   memory length: 8600   epsilon: 0.8288600000003301\n",
      "episode: 43   score: -200.0   memory length: 8800   epsilon: 0.8248800000003378\n",
      "episode: 44   score: -200.0   memory length: 9000   epsilon: 0.8209000000003455\n",
      "episode: 45   score: -200.0   memory length: 9200   epsilon: 0.8169200000003531\n",
      "episode: 46   score: -200.0   memory length: 9400   epsilon: 0.8129400000003608\n",
      "episode: 47   score: -200.0   memory length: 9600   epsilon: 0.8089600000003685\n",
      "episode: 48   score: -200.0   memory length: 9800   epsilon: 0.8049800000003762\n",
      "episode: 49   score: -200.0   memory length: 10000   epsilon: 0.8010000000003838\n",
      "episode: 50   score: -200.0   memory length: 10000   epsilon: 0.7970200000003915\n",
      "episode: 51   score: -200.0   memory length: 10000   epsilon: 0.7930400000003992\n",
      "episode: 52   score: -200.0   memory length: 10000   epsilon: 0.7890600000004069\n",
      "episode: 53   score: -200.0   memory length: 10000   epsilon: 0.7850800000004146\n",
      "episode: 54   score: -200.0   memory length: 10000   epsilon: 0.7811000000004222\n",
      "episode: 55   score: -200.0   memory length: 10000   epsilon: 0.7771200000004299\n",
      "episode: 56   score: -200.0   memory length: 10000   epsilon: 0.7731400000004376\n",
      "episode: 57   score: -200.0   memory length: 10000   epsilon: 0.7691600000004453\n",
      "episode: 58   score: -200.0   memory length: 10000   epsilon: 0.7651800000004529\n",
      "episode: 59   score: -200.0   memory length: 10000   epsilon: 0.7612000000004606\n",
      "episode: 60   score: -200.0   memory length: 10000   epsilon: 0.7572200000004683\n",
      "episode: 61   score: -200.0   memory length: 10000   epsilon: 0.753240000000476\n",
      "episode: 62   score: -200.0   memory length: 10000   epsilon: 0.7492600000004837\n",
      "episode: 63   score: -200.0   memory length: 10000   epsilon: 0.7452800000004913\n",
      "episode: 64   score: -200.0   memory length: 10000   epsilon: 0.741300000000499\n",
      "episode: 65   score: -200.0   memory length: 10000   epsilon: 0.7373200000005067\n",
      "episode: 66   score: -200.0   memory length: 10000   epsilon: 0.7333400000005144\n",
      "episode: 67   score: -200.0   memory length: 10000   epsilon: 0.729360000000522\n",
      "episode: 68   score: -200.0   memory length: 10000   epsilon: 0.7253800000005297\n",
      "episode: 69   score: -200.0   memory length: 10000   epsilon: 0.7214000000005374\n",
      "episode: 70   score: -200.0   memory length: 10000   epsilon: 0.7174200000005451\n",
      "episode: 71   score: -200.0   memory length: 10000   epsilon: 0.7134400000005527\n",
      "episode: 72   score: -170.0   memory length: 10000   epsilon: 0.7100570000005593\n",
      "episode: 73   score: -200.0   memory length: 10000   epsilon: 0.706077000000567\n",
      "episode: 74   score: -200.0   memory length: 10000   epsilon: 0.7020970000005746\n",
      "episode: 75   score: -200.0   memory length: 10000   epsilon: 0.6981170000005823\n",
      "episode: 76   score: -200.0   memory length: 10000   epsilon: 0.69413700000059\n",
      "episode: 77   score: -200.0   memory length: 10000   epsilon: 0.6901570000005977\n",
      "episode: 78   score: -200.0   memory length: 10000   epsilon: 0.6861770000006053\n",
      "episode: 79   score: -200.0   memory length: 10000   epsilon: 0.682197000000613\n",
      "episode: 80   score: -200.0   memory length: 10000   epsilon: 0.6782170000006207\n",
      "episode: 81   score: -200.0   memory length: 10000   epsilon: 0.6742370000006284\n",
      "episode: 82   score: -200.0   memory length: 10000   epsilon: 0.670257000000636\n",
      "episode: 83   score: -200.0   memory length: 10000   epsilon: 0.6662770000006437\n",
      "episode: 84   score: -187.0   memory length: 10000   epsilon: 0.6625557000006509\n",
      "episode: 85   score: -200.0   memory length: 10000   epsilon: 0.6585757000006586\n",
      "episode: 86   score: -200.0   memory length: 10000   epsilon: 0.6545957000006662\n",
      "episode: 87   score: -200.0   memory length: 10000   epsilon: 0.6506157000006739\n",
      "episode: 88   score: -200.0   memory length: 10000   epsilon: 0.6466357000006816\n",
      "episode: 89   score: -200.0   memory length: 10000   epsilon: 0.6426557000006893\n",
      "episode: 90   score: -200.0   memory length: 10000   epsilon: 0.638675700000697\n",
      "episode: 91   score: -200.0   memory length: 10000   epsilon: 0.6346957000007046\n",
      "episode: 92   score: -200.0   memory length: 10000   epsilon: 0.6307157000007123\n",
      "episode: 93   score: -200.0   memory length: 10000   epsilon: 0.62673570000072\n",
      "episode: 94   score: -200.0   memory length: 10000   epsilon: 0.6227557000007277\n",
      "episode: 95   score: -200.0   memory length: 10000   epsilon: 0.6187757000007353\n",
      "episode: 96   score: -200.0   memory length: 10000   epsilon: 0.614795700000743\n",
      "episode: 97   score: -200.0   memory length: 10000   epsilon: 0.6108157000007507\n",
      "episode: 98   score: -200.0   memory length: 10000   epsilon: 0.6068357000007584\n",
      "episode: 99   score: -200.0   memory length: 10000   epsilon: 0.602855700000766\n",
      "episode: 100   score: -200.0   memory length: 10000   epsilon: 0.5988757000007737\n",
      "episode: 101   score: -200.0   memory length: 10000   epsilon: 0.5948957000007814\n",
      "episode: 102   score: -200.0   memory length: 10000   epsilon: 0.5909157000007891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 103   score: -200.0   memory length: 10000   epsilon: 0.5869357000007968\n",
      "episode: 104   score: -200.0   memory length: 10000   epsilon: 0.5829557000008044\n",
      "episode: 105   score: -200.0   memory length: 10000   epsilon: 0.5789757000008121\n",
      "episode: 106   score: -200.0   memory length: 10000   epsilon: 0.5749957000008198\n",
      "episode: 107   score: -200.0   memory length: 10000   epsilon: 0.5710157000008275\n",
      "episode: 108   score: -200.0   memory length: 10000   epsilon: 0.5670357000008351\n",
      "episode: 109   score: -200.0   memory length: 10000   epsilon: 0.5630557000008428\n",
      "episode: 110   score: -200.0   memory length: 10000   epsilon: 0.5590757000008505\n",
      "episode: 111   score: -200.0   memory length: 10000   epsilon: 0.5550957000008582\n",
      "episode: 112   score: -200.0   memory length: 10000   epsilon: 0.5511157000008658\n",
      "episode: 113   score: -200.0   memory length: 10000   epsilon: 0.5471357000008735\n",
      "episode: 114   score: -200.0   memory length: 10000   epsilon: 0.5431557000008812\n",
      "episode: 115   score: -200.0   memory length: 10000   epsilon: 0.5391757000008889\n",
      "episode: 116   score: -200.0   memory length: 10000   epsilon: 0.5351957000008966\n",
      "episode: 117   score: -200.0   memory length: 10000   epsilon: 0.5312157000009042\n",
      "episode: 118   score: -200.0   memory length: 10000   epsilon: 0.5272357000009119\n",
      "episode: 119   score: -200.0   memory length: 10000   epsilon: 0.5232557000009196\n",
      "episode: 120   score: -200.0   memory length: 10000   epsilon: 0.5192757000009273\n",
      "episode: 121   score: -200.0   memory length: 10000   epsilon: 0.5152957000009349\n",
      "episode: 122   score: -200.0   memory length: 10000   epsilon: 0.5113157000009426\n",
      "episode: 123   score: -200.0   memory length: 10000   epsilon: 0.5073357000009503\n",
      "episode: 124   score: -200.0   memory length: 10000   epsilon: 0.503355700000958\n",
      "episode: 125   score: -200.0   memory length: 10000   epsilon: 0.4993757000009639\n",
      "episode: 126   score: -200.0   memory length: 10000   epsilon: 0.49539570000096045\n",
      "episode: 127   score: -200.0   memory length: 10000   epsilon: 0.491415700000957\n",
      "episode: 128   score: -200.0   memory length: 10000   epsilon: 0.4874357000009536\n",
      "episode: 129   score: -200.0   memory length: 10000   epsilon: 0.4834557000009502\n",
      "episode: 130   score: -200.0   memory length: 10000   epsilon: 0.47947570000094675\n",
      "episode: 131   score: -200.0   memory length: 10000   epsilon: 0.4754957000009433\n",
      "episode: 132   score: -200.0   memory length: 10000   epsilon: 0.4715157000009399\n",
      "episode: 133   score: -200.0   memory length: 10000   epsilon: 0.46753570000093647\n",
      "episode: 134   score: -200.0   memory length: 10000   epsilon: 0.46355570000093305\n",
      "episode: 135   score: -200.0   memory length: 10000   epsilon: 0.4595757000009296\n",
      "episode: 136   score: -200.0   memory length: 10000   epsilon: 0.4555957000009262\n",
      "episode: 137   score: -200.0   memory length: 10000   epsilon: 0.45161570000092277\n",
      "episode: 138   score: -200.0   memory length: 10000   epsilon: 0.44763570000091935\n",
      "episode: 139   score: -200.0   memory length: 10000   epsilon: 0.4436557000009159\n",
      "episode: 140   score: -200.0   memory length: 10000   epsilon: 0.4396757000009125\n",
      "episode: 141   score: -200.0   memory length: 10000   epsilon: 0.43569570000090907\n",
      "episode: 142   score: -200.0   memory length: 10000   epsilon: 0.43171570000090564\n",
      "episode: 143   score: -200.0   memory length: 10000   epsilon: 0.4277357000009022\n",
      "episode: 144   score: -200.0   memory length: 10000   epsilon: 0.4237557000008988\n",
      "episode: 145   score: -200.0   memory length: 10000   epsilon: 0.41977570000089537\n",
      "episode: 146   score: -200.0   memory length: 10000   epsilon: 0.41579570000089194\n",
      "episode: 147   score: -200.0   memory length: 10000   epsilon: 0.4118157000008885\n",
      "episode: 148   score: -200.0   memory length: 10000   epsilon: 0.4078357000008851\n",
      "episode: 149   score: -200.0   memory length: 10000   epsilon: 0.40385570000088167\n",
      "episode: 150   score: -200.0   memory length: 10000   epsilon: 0.39987570000087824\n",
      "episode: 151   score: -200.0   memory length: 10000   epsilon: 0.3958957000008748\n",
      "episode: 152   score: -200.0   memory length: 10000   epsilon: 0.3919157000008714\n",
      "episode: 153   score: -200.0   memory length: 10000   epsilon: 0.38793570000086797\n",
      "episode: 154   score: -200.0   memory length: 10000   epsilon: 0.38395570000086454\n",
      "episode: 155   score: -200.0   memory length: 10000   epsilon: 0.3799757000008611\n",
      "episode: 156   score: -200.0   memory length: 10000   epsilon: 0.3759957000008577\n",
      "episode: 157   score: -200.0   memory length: 10000   epsilon: 0.37201570000085427\n",
      "episode: 158   score: -200.0   memory length: 10000   epsilon: 0.36803570000085084\n",
      "episode: 159   score: -200.0   memory length: 10000   epsilon: 0.3640557000008474\n",
      "episode: 160   score: -200.0   memory length: 10000   epsilon: 0.360075700000844\n",
      "episode: 161   score: -200.0   memory length: 10000   epsilon: 0.35609570000084056\n",
      "episode: 162   score: -200.0   memory length: 10000   epsilon: 0.35211570000083714\n",
      "episode: 163   score: -200.0   memory length: 10000   epsilon: 0.3481357000008337\n",
      "episode: 164   score: -200.0   memory length: 10000   epsilon: 0.3441557000008303\n",
      "episode: 165   score: -200.0   memory length: 10000   epsilon: 0.34017570000082686\n",
      "episode: 166   score: -200.0   memory length: 10000   epsilon: 0.33619570000082344\n",
      "episode: 167   score: -200.0   memory length: 10000   epsilon: 0.33221570000082\n",
      "episode: 168   score: -200.0   memory length: 10000   epsilon: 0.3282357000008166\n",
      "episode: 169   score: -200.0   memory length: 10000   epsilon: 0.32425570000081316\n",
      "episode: 170   score: -200.0   memory length: 10000   epsilon: 0.32027570000080974\n",
      "episode: 171   score: -166.0   memory length: 10000   epsilon: 0.3169723000008069\n",
      "episode: 172   score: -155.0   memory length: 10000   epsilon: 0.31388780000080424\n",
      "episode: 173   score: -200.0   memory length: 10000   epsilon: 0.3099078000008008\n",
      "episode: 174   score: -200.0   memory length: 10000   epsilon: 0.3059278000007974\n",
      "episode: 175   score: -200.0   memory length: 10000   epsilon: 0.30194780000079396\n",
      "episode: 176   score: -200.0   memory length: 10000   epsilon: 0.29796780000079054\n",
      "episode: 177   score: -200.0   memory length: 10000   epsilon: 0.2939878000007871\n",
      "episode: 178   score: -176.0   memory length: 10000   epsilon: 0.2904854000007841\n",
      "episode: 179   score: -152.0   memory length: 10000   epsilon: 0.2874606000007815\n",
      "episode: 180   score: -200.0   memory length: 10000   epsilon: 0.28348060000077807\n",
      "episode: 181   score: -200.0   memory length: 10000   epsilon: 0.27950060000077465\n",
      "episode: 182   score: -200.0   memory length: 10000   epsilon: 0.2755206000007712\n",
      "episode: 183   score: -200.0   memory length: 10000   epsilon: 0.2715406000007678\n",
      "episode: 184   score: -199.0   memory length: 10000   epsilon: 0.2675805000007644\n",
      "episode: 185   score: -172.0   memory length: 10000   epsilon: 0.26415770000076144\n",
      "episode: 186   score: -99.0   memory length: 10000   epsilon: 0.26218760000075975\n",
      "episode: 187   score: -159.0   memory length: 10000   epsilon: 0.259023500000757\n",
      "episode: 188   score: -200.0   memory length: 10000   epsilon: 0.2550435000007536\n",
      "episode: 189   score: -200.0   memory length: 10000   epsilon: 0.2510635000007502\n",
      "episode: 190   score: -200.0   memory length: 10000   epsilon: 0.24708350000075083\n",
      "episode: 191   score: -152.0   memory length: 10000   epsilon: 0.24405870000075244\n",
      "episode: 192   score: -171.0   memory length: 10000   epsilon: 0.24065580000075426\n",
      "episode: 193   score: -187.0   memory length: 10000   epsilon: 0.23693450000075625\n",
      "episode: 194   score: -170.0   memory length: 10000   epsilon: 0.23355150000075806\n",
      "episode: 195   score: -149.0   memory length: 10000   epsilon: 0.23058640000075964\n",
      "episode: 196   score: -181.0   memory length: 10000   epsilon: 0.22698450000076156\n",
      "episode: 197   score: -200.0   memory length: 10000   epsilon: 0.2230045000007637\n",
      "episode: 198   score: -191.0   memory length: 10000   epsilon: 0.21920360000076572\n",
      "episode: 199   score: -165.0   memory length: 10000   epsilon: 0.21592010000076747\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "#     print(state)\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 4 times\n",
    "    action_count = 0\n",
    "    \n",
    "#     actions = []\n",
    "\n",
    "    while not done:\n",
    "#         if agent.render:\n",
    "#             env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "\n",
    "#         fake_action = agent.get_action(state)\n",
    "#         actions.append(fake_action)\n",
    "        \n",
    "        if action_count == 4:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "\n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                  \"  epsilon:\", agent.epsilon)\n",
    "#             print(np.unique(np.array(actions), return_counts=True))\n",
    "\n",
    "    # Save model for every 50 episodes\n",
    "    if e % 50 == 0:\n",
    "        agent.save_model(\"./save_model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
