{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    \"\"\"\n",
    "    The movie review dataset is offered by Stanford Universityâ€™s AI department:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/. It comes as a compressed  tar  archive where\n",
    "    positive and negative reviews can be found as text files in two according folders. We apply\n",
    "    the same pre-processing to the text as in the last section: Extracting plain words using a\n",
    "    regular expression and converting to lower case.\n",
    "    \"\"\"\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = './imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "        self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence):\n",
    "        data = np.zeros((self._length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SequenceClassificationModel:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self._create_placeholders()\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        self.global_step = 0\n",
    "        self._create_summaries()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.data = tf.placeholder(tf.float32, [None, self.params.seq_length, self.params.embed_length])\n",
    "            self.target = tf.placeholder(tf.float32, [None, 2])\n",
    "  \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "            tf.summary.scalar('erroe', self.error)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "    @lazy\n",
    "    def length(self):\n",
    "        with tf.name_scope(\"seq_length\"):\n",
    "            used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "            length = tf.reduce_sum(used, reduction_indices=1)\n",
    "            length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy\n",
    "    def prediction(self):\n",
    "        with tf.name_scope(\"recurrent_layer\"):\n",
    "            output, _ = tf.nn.dynamic_rnn(\n",
    "                self.params.rnn_cell(self.params.rnn_hidden),\n",
    "                self.data,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.length\n",
    "            )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            num_classes = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal(\n",
    "                [self.params.rnn_hidden, num_classes], stddev=0.01))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    @lazy\n",
    "    def error(self):\n",
    "        self.mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "    \n",
    "    @lazy\n",
    "    def optimize(self):\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            gradient = self.params.optimizer.compute_gradients(self.cost)\n",
    "            if self.params.gradient_clipping:\n",
    "                limit = self.params.gradient_clipping\n",
    "                gradient = [\n",
    "                    (tf.clip_by_value(g, -limit, limit), v)\n",
    "                    if g is not None else (None, v)\n",
    "                    for g, v in gradient]\n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "        return optimize\n",
    "    \n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        with tf.name_scope(\"last_relevant\"):\n",
    "            # As of now, TensorFlow only supports indexing along the first dimension, using\n",
    "            # tf.gather() . We thus flatten the first two dimensions of the output activations from their\n",
    "            # shape of  sequences x time_steps x word_vectors  and construct an index into this resulting tensor.\n",
    "            batch_size = tf.shape(output)[0]\n",
    "            max_length = int(output.get_shape()[1])\n",
    "            output_size = int(output.get_shape()[2])\n",
    "\n",
    "            # The index takes into account the start indices for each sequence in the flat tensor and adds\n",
    "            # the sequence length to it. Actually, we only add  length - 1  so that we select the last valid\n",
    "            # time step.\n",
    "            index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "            flat = tf.reshape(output, [-1, output_size])\n",
    "            relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "    \n",
    "    def train(self, batches, save_prefix, save_every=10):\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.isdir('./saved/'):\n",
    "            saver.restore(self.sess, tf.train.latest_checkpoint('./saved/'))\n",
    "        else:\n",
    "            os.makedirs('saved')\n",
    "        summary_writer = tf.summary.FileWriter('graphs/run{}'.format(self.global_step), self.sess.graph)\n",
    "        self.global_step += 1\n",
    "        for index, batch in enumerate(batches):\n",
    "            feed = {model.data: batch[0], model.target: batch[1]}\n",
    "            error, _, summary_str = self.sess.run([model.error, model.optimize, model.summary], feed)\n",
    "            print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "            if index % save_every == 0:\n",
    "                summary_writer.add_summary(summary_str, index)\n",
    "                summary_writer.flush()\n",
    "            if index % save_every == 0:\n",
    "                save_path = os.path.join('checkpoints', save_prefix)\n",
    "                print('saving...', save_path)\n",
    "                saver.save(self.sess, save_path, global_step=index)\n",
    "        saver.save(self.sess, os.path.join('checkpoints', save_prefix + '_final'))\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        feed = {model.data: data, }\n",
    "        prediction = self.sess.run([model.prediction], feed)        \n",
    "        return prediction\n",
    "        \n",
    "    def close(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        data = np.zeros((batch_size, length, embedding.dimensions))\n",
    "        target = np.zeros((batch_size, 2))\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            data[index] = embedding(text)\n",
    "            target[index] = [1, 0] if label else [0, 1]\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = max(len(x[0]) for x in reviews)\n",
    "embedding = Embedding(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "import tensorflow as tf\n",
    "\n",
    "params = AttrDict(\n",
    "    rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "    rnn_hidden=300,\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "    batch_size=20,\n",
    "    gradient_clipping=100,\n",
    "    seq_length=length,\n",
    "    embed_length=embedding.dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = preprocess_batched(reviews, length, embedding, params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SequenceClassificationModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(batches, save_prefix='simple-rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from no_att_checkpoints/simple-rnn-1240\n",
      "INFO:tensorflow:Restoring parameters from no_att_checkpoints/simple-rnn-1240\n",
      "0 90.0\n",
      "25 87.6923076923\n",
      "50 88.7254901961\n",
      "75 88.4210526316\n",
      "100 88.4158415842\n",
      "125 88.5317460317\n",
      "150 88.4768211921\n",
      "175 88.2954545455\n",
      "200 88.2089552239\n",
      "225 88.0973451327\n",
      "250 88.0278884462\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.get_checkpoint_state('no_att_checkpoints')\n",
    "\n",
    "if checkpoint:\n",
    "    \n",
    "    print(\"Reading model parameters from %s\" % checkpoint.model_checkpoint_path)\n",
    "    saver.restore(model.sess, checkpoint.model_checkpoint_path)\n",
    "\n",
    "    correct_num = 0\n",
    "    \n",
    "    for t, (data, target) in enumerate(batches):\n",
    "        \n",
    "        if t > 250:\n",
    "            break\n",
    "\n",
    "        fd = {model.data: data}\n",
    "        \n",
    "        prediction = np.argmax(model.sess.run(model.prediction, fd), axis=1)\n",
    "        \n",
    "        correct_num += np.sum(prediction == np.argmax(target, axis=1))\n",
    "\n",
    "        if t % 25 == 0:\n",
    "            print(t, 5 * correct_num / (t + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
