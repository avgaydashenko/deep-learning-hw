{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = './imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "#         self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence, length):\n",
    "        # DO I really need them to be equal length?\n",
    "        # Let's assume I'm not\n",
    "        data = np.zeros((length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def preprocess_batched_split(iterator, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        batch = []\n",
    "        labelss = []\n",
    "        sentence_sizes_batch = []\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            sents = [list(y) for x, y in itertools.groupby(text, lambda z: z == '.') if not x]\n",
    "            sentence_sizes = [len(s) for s in sents]\n",
    "            text_embed = [embedding(sent) for sent in sents]\n",
    "            \n",
    "            batch.append(text_embed)\n",
    "            labelss.append(label)\n",
    "            sentence_sizes_batch.append(sentence_sizes)\n",
    "            \n",
    "        labels_batch = np.array(labelss, dtype=np.int32)\n",
    "        sent_per_doc = np.array([len(x) for x in sentence_sizes_batch])\n",
    "        words_per_sent_per_doc = np.array(sentence_sizes_batch)\n",
    "        yield np.array(batch), labels_batch, words_per_sent_per_doc, sent_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def preprocess_batched_split2(iterator, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        batch, labels_b = zip(*itertools.islice(iterator, batch_size))\n",
    "        \n",
    "        sents_b = [[list(y) for x, y in itertools.groupby(doc, lambda z: z == '.') if not x] for doc in batch]\n",
    "\n",
    "        sentence_sizes_b = [[len(sent) for sent in doc] for doc in sents_b]\n",
    "        sentence_size = max(map(max, sentence_sizes_b))\n",
    "        \n",
    "        document_sizes = np.array([len(doc) for doc in sentence_sizes_b], dtype=np.int32)\n",
    "        document_size = document_sizes.max()\n",
    "\n",
    "        sentence_sizes_np = np.zeros(shape=[batch_size, document_size], dtype=np.int32)\n",
    "        for bi, ds, ss in zip(range(sentence_sizes_np.shape[0]), document_sizes, sentence_sizes_b):\n",
    "            sentence_sizes_np[bi][:ds] = ss\n",
    "        \n",
    "        text_embed_b = np.zeros((batch_size, document_size, sentence_size, 300))\n",
    "        for i, ds, doc_sents in zip(range(text_embed_b.shape[0]), document_sizes, sents_b):\n",
    "            doc_sents_embed = np.array([embedding(sent, sentence_size) for sent in doc_sents])\n",
    "            text_embed_b[i][:ds] = doc_sents_embed\n",
    "        \n",
    "        yield text_embed_b, np.array(labels_b, dtype=np.int32), np.array(document_sizes), sentence_sizes_np, sents_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "HanSequenceLabellingModel model_components\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport HanSequenceLabellingModel, model_components\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_split = preprocess_batched_split2(reviews, Embedding(), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HanSequenceLabellingModel import HanSequenceLabellingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAN_model_1(session, restore_only=False):\n",
    "    \"\"\"Hierarhical Attention Network\"\"\"\n",
    "    import tensorflow as tf\n",
    "    try:\n",
    "        from tensorflow.contrib.rnn import GRUCell, MultiRNNCell, DropoutWrapper\n",
    "    except ImportError:\n",
    "        MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n",
    "        GRUCell = tf.nn.rnn_cell.GRUCell\n",
    "    from bn_lstm import BNLSTMCell\n",
    "    from HanSequenceLabellingModel import HanSequenceLabellingModel\n",
    "\n",
    "    is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "    cell = BNLSTMCell(80, is_training) # h-h batchnorm LSTMCell\n",
    "    cell = MultiRNNCell([cell]*5)\n",
    "\n",
    "    model = HanSequenceLabellingModel(\n",
    "            embedding_size=300,\n",
    "            classes=2,\n",
    "            word_cell=cell,\n",
    "            sentence_cell=cell,\n",
    "            word_output_size=300,\n",
    "            sentence_output_size=300,\n",
    "            learning_rate=0.001,\n",
    "            max_grad_norm=5.0,\n",
    "            dropout_keep_proba=0.5,\n",
    "            is_training=is_training,\n",
    "    )\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if checkpoint:\n",
    "        print(\"Reading model parameters from %s\" % checkpoint.model_checkpoint_path)\n",
    "        saver.restore(session, checkpoint.model_checkpoint_path)\n",
    "    elif restore_only:\n",
    "        raise FileNotFoundError(\"Cannot restore model\")\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    return model, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoints/checkpoint-2400\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoint-2400\n",
      "\u001b[30;48;2;255;130;130m   \u001b[0m \u001b[30;48;2;174;174;255mi\u001b[0m \u001b[30;48;2;198;198;255msaw\u001b[0m \u001b[30;48;2;182;182;255mthis\u001b[0m \u001b[30;48;2;155;155;255mmovie\u001b[0m \u001b[30;48;2;170;170;255mtwo\u001b[0m \u001b[30;48;2;202;202;255mweeks\u001b[0m \u001b[30;48;2;219;219;255mago\u001b[0m \u001b[30;48;2;225;225;255mat\u001b[0m \u001b[30;48;2;225;225;255mthe\u001b[0m \u001b[30;48;2;220;220;255mfestival\u001b[0m \u001b[30;48;2;220;220;255mdes\u001b[0m \u001b[30;48;2;219;219;255mnouvelles\u001b[0m \u001b[30;48;2;210;210;255mimages\u001b[0m \u001b[30;48;2;206;206;255mdu\u001b[0m \u001b[30;48;2;205;205;255mjapon\u001b[0m \u001b[30;48;2;212;212;255min\u001b[0m \u001b[30;48;2;211;211;255mparis\u001b[0m \n",
      "\u001b[30;48;2;255;124;124m   \u001b[0m \u001b[30;48;2;155;155;255mthough\u001b[0m \u001b[30;48;2;159;159;255mi\u001b[0m \u001b[30;48;2;169;169;255mwasn\u001b[0m \u001b[30;48;2;188;188;255mt\u001b[0m \u001b[30;48;2;206;206;255mexpecting\u001b[0m \u001b[30;48;2;221;221;255mmuch\u001b[0m \u001b[30;48;2;218;218;255mfrom\u001b[0m \u001b[30;48;2;206;206;255mit\u001b[0m \u001b[30;48;2;184;184;255m,\u001b[0m \u001b[30;48;2;161;161;255mi\u001b[0m \u001b[30;48;2;186;186;255mhave\u001b[0m \u001b[30;48;2;205;205;255mto\u001b[0m \u001b[30;48;2;203;203;255msay\u001b[0m \u001b[30;48;2;202;202;255mi\u001b[0m \u001b[30;48;2;191;191;255mve\u001b[0m \u001b[30;48;2;196;196;255mbeen\u001b[0m \u001b[30;48;2;187;187;255mdisappointed\u001b[0m \u001b[30;48;2;198;198;255mjust\u001b[0m \u001b[30;48;2;207;207;255mlike\u001b[0m \u001b[30;48;2;216;216;255mmany\u001b[0m \u001b[30;48;2;219;219;255mpeople\u001b[0m \u001b[30;48;2;219;219;255min\u001b[0m \u001b[30;48;2;215;215;255mthe\u001b[0m \u001b[30;48;2;212;212;255maudience\u001b[0m \n",
      "\u001b[30;48;2;255;136;136m   \u001b[0m \u001b[30;48;2;187;187;255mif\u001b[0m \u001b[30;48;2;196;196;255mi\u001b[0m \u001b[30;48;2;200;200;255mwanted\u001b[0m \u001b[30;48;2;207;207;255mto\u001b[0m \u001b[30;48;2;210;210;255msum\u001b[0m \u001b[30;48;2;209;209;255mup\u001b[0m \u001b[30;48;2;206;206;255mhow\u001b[0m \u001b[30;48;2;188;188;255mi\u001b[0m \u001b[30;48;2;184;184;255mfelt\u001b[0m \u001b[30;48;2;179;179;255m,\u001b[0m \u001b[30;48;2;156;156;255mi\u001b[0m \u001b[30;48;2;155;155;255md\u001b[0m \u001b[30;48;2;168;168;255msay\u001b[0m \u001b[30;48;2;185;185;255mi\u001b[0m \u001b[30;48;2;180;180;255mve\u001b[0m \u001b[30;48;2;190;190;255mbeen\u001b[0m \u001b[30;48;2;192;192;255mcomparing\u001b[0m \u001b[30;48;2;178;178;255mit\u001b[0m \u001b[30;48;2;180;180;255mto\u001b[0m \u001b[30;48;2;167;167;255mprincess\u001b[0m \u001b[30;48;2;175;175;255mmononoke\u001b[0m \u001b[30;48;2;166;166;255mand\u001b[0m \u001b[30;48;2;165;165;255mnausicaa\u001b[0m \u001b[30;48;2;169;169;255mfrom\u001b[0m \u001b[30;48;2;168;168;255mthe\u001b[0m \u001b[30;48;2;170;170;255mbeginning\u001b[0m \u001b[30;48;2;177;177;255mto\u001b[0m \u001b[30;48;2;172;172;255mthe\u001b[0m \u001b[30;48;2;163;163;255mend\u001b[0m \n",
      "\u001b[30;48;2;255;104;104m   \u001b[0m \u001b[30;48;2;226;226;255mof\u001b[0m \u001b[30;48;2;233;233;255mcourse\u001b[0m \u001b[30;48;2;229;229;255mit\u001b[0m \u001b[30;48;2;217;217;255ms\u001b[0m \u001b[30;48;2;155;155;255msilly\u001b[0m \n",
      "\u001b[30;48;2;255;128;128m   \u001b[0m \u001b[30;48;2;155;155;255mbut\u001b[0m \u001b[30;48;2;171;171;255mi\u001b[0m \u001b[30;48;2;181;181;255mcouldn\u001b[0m \u001b[30;48;2;196;196;255mt\u001b[0m \u001b[30;48;2;218;218;255mhelp\u001b[0m \u001b[30;48;2;208;208;255mit\u001b[0m \n",
      "\u001b[30;48;2;255;154;154m   \u001b[0m \u001b[30;48;2;171;171;255mthe\u001b[0m \u001b[30;48;2;162;162;255mstories\u001b[0m \u001b[30;48;2;179;179;255mare\u001b[0m \u001b[30;48;2;189;189;255mquite\u001b[0m \u001b[30;48;2;197;197;255mdifferent\u001b[0m \u001b[30;48;2;190;190;255m,\u001b[0m \u001b[30;48;2;189;189;255mbut\u001b[0m \u001b[30;48;2;190;190;255mthe\u001b[0m \u001b[30;48;2;166;166;255mworlds\u001b[0m \u001b[30;48;2;155;155;255mpictured\u001b[0m \u001b[30;48;2;185;185;255mare\u001b[0m \u001b[30;48;2;177;177;255mvery\u001b[0m \u001b[30;48;2;194;194;255mmuch\u001b[0m \u001b[30;48;2;200;200;255malike\u001b[0m \n",
      "\u001b[30;48;2;255;145;145m   \u001b[0m \u001b[30;48;2;155;155;255mand\u001b[0m \u001b[30;48;2;173;173;255mfrom\u001b[0m \u001b[30;48;2;156;156;255mthis\u001b[0m \u001b[30;48;2;177;177;255mpoint\u001b[0m \u001b[30;48;2;186;186;255mof\u001b[0m \u001b[30;48;2;193;193;255mview\u001b[0m \u001b[30;48;2;189;189;255m,\u001b[0m \u001b[30;48;2;188;188;255ma\u001b[0m \u001b[30;48;2;178;178;255mtree\u001b[0m \u001b[30;48;2;185;185;255mof\u001b[0m \u001b[30;48;2;183;183;255mpalme\u001b[0m \u001b[30;48;2;157;157;255mdefinitely\u001b[0m \u001b[30;48;2;167;167;255mcan\u001b[0m \u001b[30;48;2;190;190;255mt\u001b[0m \u001b[30;48;2;206;206;255mstand\u001b[0m \u001b[30;48;2;215;215;255mthe\u001b[0m \u001b[30;48;2;201;201;255mcomparison\u001b[0m \u001b[30;48;2;214;214;255mwith\u001b[0m \u001b[30;48;2;214;214;255mmiyazaki\u001b[0m \u001b[30;48;2;214;214;255ms\u001b[0m \u001b[30;48;2;210;210;255mmasterworks\u001b[0m \n",
      "\u001b[30;48;2;255;142;142m   \u001b[0m \u001b[30;48;2;194;194;255meven\u001b[0m \u001b[30;48;2;202;202;255mif\u001b[0m \u001b[30;48;2;186;186;255mit\u001b[0m \u001b[30;48;2;186;186;255ms\u001b[0m \u001b[30;48;2;174;174;255mquite\u001b[0m \u001b[30;48;2;165;165;255mgood\u001b[0m \u001b[30;48;2;156;156;255mtechnically\u001b[0m \u001b[30;48;2;156;156;255m,\u001b[0m \u001b[30;48;2;165;165;255mboredom\u001b[0m \u001b[30;48;2;155;155;255mremains\u001b[0m \n",
      "\u001b[30;48;2;255;0;0m   \u001b[0m \u001b[30;48;2;240;240;255min\u001b[0m \u001b[30;48;2;231;231;255mthe\u001b[0m \u001b[30;48;2;213;213;255mend\u001b[0m \u001b[30;48;2;186;186;255mits\u001b[0m \u001b[30;48;2;180;180;255mcomplete\u001b[0m \u001b[30;48;2;155;155;255mlack\u001b[0m \u001b[30;48;2;224;224;255mof\u001b[0m \u001b[30;48;2;228;228;255moriginality\u001b[0m \u001b[30;48;2;228;228;255mmakes\u001b[0m \u001b[30;48;2;231;231;255mme\u001b[0m \u001b[30;48;2;237;237;255madvise\u001b[0m \u001b[30;48;2;239;239;255myou\u001b[0m \u001b[30;48;2;239;239;255mnot\u001b[0m \u001b[30;48;2;243;243;255mto\u001b[0m \u001b[30;48;2;243;243;255mcare\u001b[0m \u001b[30;48;2;243;243;255mto\u001b[0m \u001b[30;48;2;240;240;255mwatch\u001b[0m \u001b[30;48;2;234;234;255mit\u001b[0m \n",
      "\u001b[30;48;2;255;17;17m   \u001b[0m \u001b[30;48;2;181;181;255mi\u001b[0m \u001b[30;48;2;155;155;255mrated\u001b[0m \u001b[30;48;2;175;175;255mit\u001b[0m \u001b[30;48;2;197;197;255mout\u001b[0m \u001b[30;48;2;209;209;255mof\u001b[0m \u001b[30;48;2;221;221;255m(\u001b[0m \u001b[30;48;2;225;225;255ma\u001b[0m \u001b[30;48;2;224;224;255mbit\u001b[0m \u001b[30;48;2;226;226;255mharsh\u001b[0m \u001b[30;48;2;222;222;255m,\u001b[0m \u001b[30;48;2;216;216;255mi\u001b[0m \u001b[30;48;2;208;208;255mguess\u001b[0m \u001b[30;48;2;202;202;255mit\u001b[0m \u001b[30;48;2;170;170;255mdeserves\u001b[0m \u001b[30;48;2;197;197;255mor\u001b[0m \u001b[30;48;2;220;220;255m)\u001b[0m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from colors import color\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "with tf.Session(config=config) as s:\n",
    "    model, saver = HAN_model_1(s)\n",
    "\n",
    "    for data, labels_batch, sent_per_doc, words_per_sent_per_doc, sents_b in batches_split:\n",
    "\n",
    "        fd = {\n",
    "            model.is_training: True,\n",
    "            model.inputs_embedded: data,\n",
    "            model.word_lengths: words_per_sent_per_doc,\n",
    "            model.sentence_lengths: sent_per_doc,\n",
    "            model.labels: labels_batch,\n",
    "            model.sample_weights: np.ones(shape=(10))\n",
    "        }\n",
    "\n",
    "        word_attention, sentence_attention = s.run([model.word_attention, model.sentence_attention], feed_dict=fd)\n",
    "        \n",
    "        for i, review in enumerate(sents_b):\n",
    "            for j, sentence in enumerate(review): \n",
    "                capacity = 255 - int(255 * sentence_attention[i, j, 0] / np.max(sentence_attention[i]))\n",
    "                print(color('   ', 'black', '#ff{cap:02x}{cap:02x}'.format(cap=capacity)), end=' ')\n",
    "                for k, word in enumerate(sentence):\n",
    "                    capacity = 255 - int(100 * word_attention[i * 25 + j, k, 0] / np.max(word_attention[i * 25 + j]))\n",
    "                    print(color(word, 'black', '#{cap:02x}{cap:02x}ff'.format(cap=capacity)), end=' ')\n",
    "                print()\n",
    "            print()\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_split = preprocess_batched_split2(reviews, Embedding(), batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
